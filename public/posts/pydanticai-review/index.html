<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>PydanticAI - my favorite Python library of the summer | Digital Reservoirs</title><meta name=keywords content><meta name=description content="PydanticAI abstracts away much of the glue code for working with Large Language models - freeing developers to focus on business logic and use cases
Like many tech companies in 2025, my organization is experimenting with ways to leverage Generative AI in our offerings to customers. I am part of a small team working on these ideas - we’ve built our first cloud app, a chatbot using a RAG pipeline to assist users with using our software’s command line scripts, and are in the middle of making this available to clients. For our next offering, we aimed to explore “tool use” with LLMs, giving the LLM agent the ability to call selected functions to enable it to help users in a more detailed fashion."><meta name=author content="Dan Costello"><link rel=canonical href=http://localhost:1313/posts/pydanticai-review/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css integrity="sha256-j+ECM6cGvIfy4Is8+XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/pydanticai-review/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="http://localhost:1313/posts/pydanticai-review/"><meta property="og:site_name" content="Digital Reservoirs"><meta property="og:title" content="PydanticAI - my favorite Python library of the summer"><meta property="og:description" content="PydanticAI abstracts away much of the glue code for working with Large Language models - freeing developers to focus on business logic and use cases Like many tech companies in 2025, my organization is experimenting with ways to leverage Generative AI in our offerings to customers. I am part of a small team working on these ideas - we’ve built our first cloud app, a chatbot using a RAG pipeline to assist users with using our software’s command line scripts, and are in the middle of making this available to clients. For our next offering, we aimed to explore “tool use” with LLMs, giving the LLM agent the ability to call selected functions to enable it to help users in a more detailed fashion."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-08-10T13:39:43-04:00"><meta property="article:modified_time" content="2025-08-10T13:39:43-04:00"><meta property="og:image" content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="PydanticAI - my favorite Python library of the summer"><meta name=twitter:description content="PydanticAI abstracts away much of the glue code for working with Large Language models - freeing developers to focus on business logic and use cases
Like many tech companies in 2025, my organization is experimenting with ways to leverage Generative AI in our offerings to customers. I am part of a small team working on these ideas - we’ve built our first cloud app, a chatbot using a RAG pipeline to assist users with using our software’s command line scripts, and are in the middle of making this available to clients. For our next offering, we aimed to explore “tool use” with LLMs, giving the LLM agent the ability to call selected functions to enable it to help users in a more detailed fashion."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"PydanticAI - my favorite Python library of the summer","item":"http://localhost:1313/posts/pydanticai-review/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"PydanticAI - my favorite Python library of the summer","name":"PydanticAI - my favorite Python library of the summer","description":"PydanticAI abstracts away much of the glue code for working with Large Language models - freeing developers to focus on business logic and use cases Like many tech companies in 2025, my organization is experimenting with ways to leverage Generative AI in our offerings to customers. I am part of a small team working on these ideas - we’ve built our first cloud app, a chatbot using a RAG pipeline to assist users with using our software’s command line scripts, and are in the middle of making this available to clients. For our next offering, we aimed to explore “tool use” with LLMs, giving the LLM agent the ability to call selected functions to enable it to help users in a more detailed fashion.\n","keywords":[],"articleBody":"PydanticAI abstracts away much of the glue code for working with Large Language models - freeing developers to focus on business logic and use cases Like many tech companies in 2025, my organization is experimenting with ways to leverage Generative AI in our offerings to customers. I am part of a small team working on these ideas - we’ve built our first cloud app, a chatbot using a RAG pipeline to assist users with using our software’s command line scripts, and are in the middle of making this available to clients. For our next offering, we aimed to explore “tool use” with LLMs, giving the LLM agent the ability to call selected functions to enable it to help users in a more detailed fashion.\nWhen looking ahead to our next avenues for development, I took some time to review the current development libraries available for use. When I read that the team behind Pydantic was releasing a new library for interfacing with large language models, I was skeptical. Pydantic offers a great development experience for type validation, but a LLM library seemed like a big jump for them. After working with the library for a few days, I am impressed with their design choices and wish this had been available a year ago.\nSwitching between Models When reading through the Pydantic “Getting Started” section, I was very impressed with how they’d provided access to numerous LLMs with minimal configuration.\nProviding the optionality to switch between various LLM providers had led to complexity in our chat bot application - each LLM provider has their own message structure, parameters, and API endpoints that we had to handle.\nWhen using Pydantic AI, we can just provide a model string name, and we can pass messages, system prompts, and settings in a uniform method. They support OpenAI and Anthropic direct connections, as well as options such as AWS Bedrock or self-hosted ollama implementations.\nHere’s an example of pseudocode before supporting both OpenAI and Anthropic APIs1:\n... available_models = ['4o', 'sonnet3.7','qwen'] # Hidden: User selects \"model\" and defines USER_QUERY via CLI if model == \"4o\": client=OpenAI(api_key=\"KEY\") response = client.chat.completions.create( model=\"claude-opus-4-1-20250805\", # Anthropic model name messages=[ {\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": USER_QUERY} ], ) print(response) elif model == \"sonnet3.7\": client = anthropic.Anthropic(api_key=\"my_api_key\") response = client.messages.create( model=\"claude-3-7-sonnet-20250219\", system=SYSTEM_PROMPT, messages=[ {\"role\": \"user\", \"content\": USER_QUERY} ] ) print(response) elif model == \"qwen\": ... The configuration becomes more complicating when providing optionality for self-hosted models or those on AWS Bedrock.\nAnd with Pydantic AI, we can just switch out the model name:\nfrom pydantic_ai import Agent model = 'google-gla:gemini-1.5-flash' model = 'anthropic:claude-3-7-sonnet-latest' selected_model = 'bedrock:us.anthropic.claude-3-7-sonnet-20250219-v1:0' agent = Agent( selected_model, system_prompt=SYSTEM_PROMPT, ) result = agent.run_sync('Where does \"hello world\" come from?') print(result.output) Tool Use This pleased me enough, but there was more coming when testing tool use. With most of the LLM SDKs, the tool use section of the agent initiation is a complex nest of JSON. With Pydantic AI, I can just throw a decorator above a function, and the agent now has access to this tool. We can also provide context (such as a database connection) to these tools, further empowering the LLMs.\nHere’s a comparison of tool use with the Anthropic SDK compared with Pydantic AI:\nTool use with Anthropic SDK: import anthropic import json client = anthropic.Anthropic() # Python function to get weather from API def get_weather(location, unit=\"fahrenheit\"): # In reality, you'd call a weather API here return { \"location\": location, \"temperature\": \"72\", \"unit\": unit, \"condition\": \"sunny\" } # tools schema to describe avaialble functions tools = [ { \"name\": \"get_weather\", \"description\": \"Get the current weather in a given location\", \"input_schema\": { \"type\": \"object\", \"properties\": { \"location\": { \"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\" }, \"unit\": { \"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"The unit of temperature to return\" } }, \"required\": [\"location\"] } } ] # Send request to external LLM, providing tools schema message = client.messages.create( model=\"claude-3-5-sonnet-20241022\", max_tokens=1024, tools=tools, messages=[ {\"role\": \"user\", \"content\": \"What's the weather like in Boston?\"} ] ) Tool use with PydanticAI: from pydantic_ai import Agent, RunContext agent = Agent('anthropic:claude-3-5-sonnet-latest') @agent.tool_plain def get_weather(location, unit=\"fahrenheit\"): \"\"\"Retrieve temperature and conditions for a given location.\"\" return { \"location\": location, \"temperature\": \"72\", \"unit\": unit, \"condition\": \"sunny\" } weather = agent.run_sync('What's the weather like today in San Antonio?') The decorator design makes adding tools much more straightforward and fun - we can now focus on what capabilities we want to give our agent, and not be bogged down in ensuring the JSON schema is complete for each available tool.\nWe are still early in our “tool use” era of LLM development, but it is a joy to work with Pydantic AI so we can focus on the overall program logic and user empowerment.\nWhile writing this piece I realized that Anthropic and many other LLM providers provide OpenAI-compatible endpoints, which removes the need for the if logic and accomodating the various SDK languages. This would have useful to discover earlier, but good to know! ↩︎\n","wordCount":"833","inLanguage":"en","image":"http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2025-08-10T13:39:43-04:00","dateModified":"2025-08-10T13:39:43-04:00","author":{"@type":"Person","name":"Dan Costello"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/pydanticai-review/"},"publisher":{"@type":"Organization","name":"Digital Reservoirs","logo":{"@type":"ImageObject","url":"http://localhost:1313/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Digital Reservoirs (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Digital Reservoirs</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/categories/ title=categories><span>categories</span></a></li><li><a href=http://localhost:1313/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">PydanticAI - my favorite Python library of the summer
<span class=entry-hint title=Draft><svg height="35" viewBox="0 -960 960 960" fill="currentColor"><path d="M160-410v-60h3e2v60H160zm0-165v-60h470v60H160zm0-165v-60h470v60H160zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22-4.5 22.5T862.09-380L643-160H520zm3e2-263-37-37 37 37zM580-220h38l121-122-18-19-19-18-122 121v38zm141-141-19-18 37 37-18-19z"/></svg></span></h1><div class=post-meta><span title='2025-08-10 13:39:43 -0400 EDT'>August 10, 2025</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;833 words&nbsp;·&nbsp;Dan Costello</div></header><div class=post-content><h4 id=pydanticai-abstracts-away-much-of-the-glue-code-for-working-with-large-language-models---freeing-developers-to-focus-on-business-logic-and-use-cases>PydanticAI abstracts away much of the glue code for working with Large Language models - freeing developers to focus on business logic and use cases<a hidden class=anchor aria-hidden=true href=#pydanticai-abstracts-away-much-of-the-glue-code-for-working-with-large-language-models---freeing-developers-to-focus-on-business-logic-and-use-cases>#</a></h4><p>Like many tech companies in 2025, my organization is experimenting with ways to leverage Generative AI in our offerings to customers. I am part of a small team working on these ideas - we’ve built our first cloud app, a chatbot using a RAG pipeline to assist users with using our software’s command line scripts, and are in the middle of making this available to clients. For our next offering, we aimed to explore “tool use” with LLMs, giving the LLM agent the ability to call selected functions to enable it to help users in a more detailed fashion.</p><p>When looking ahead to our next avenues for development, I took some time to review the current development libraries available for use. When I read that the team behind Pydantic was releasing a new library for interfacing with large language models, I was skeptical. Pydantic offers a great development experience for type validation, but a LLM library seemed like a big jump for them. After working with the library for a few days, I am impressed with their design choices and wish this had been available a year ago.</p><h2 id=switching-between-models>Switching between Models<a hidden class=anchor aria-hidden=true href=#switching-between-models>#</a></h2><p>When reading through the Pydantic “Getting Started” section, I was very impressed with how they’d provided access to numerous LLMs with minimal configuration.</p><p>Providing the optionality to switch between various LLM providers had led to complexity in our chat bot application - each LLM provider has their own message structure, parameters, and API endpoints that we had to handle.</p><p>When using Pydantic AI, we can just provide a model string name, and we can pass messages, system prompts, and settings in a uniform method. They support OpenAI and Anthropic direct connections, as well as options such as AWS Bedrock or self-hosted ollama implementations.</p><p>Here’s an example of pseudocode before supporting both OpenAI and Anthropic APIs<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>:</p><pre tabindex=0><code>...
available_models = [&#39;4o&#39;, &#39;sonnet3.7&#39;,&#39;qwen&#39;]

# Hidden: User selects &#34;model&#34; and defines USER_QUERY via CLI

if model == &#34;4o&#34;:
    client=OpenAI(api_key=&#34;KEY&#34;)
    response = client.chat.completions.create(
        model=&#34;claude-opus-4-1-20250805&#34;, # Anthropic model name
        messages=[
            {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: SYSTEM_PROMPT},
            {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: USER_QUERY}
        ],
    )
    print(response)
elif model == &#34;sonnet3.7&#34;:
    client = anthropic.Anthropic(api_key=&#34;my_api_key&#34;)
    response = client.messages.create(
        model=&#34;claude-3-7-sonnet-20250219&#34;,
        system=SYSTEM_PROMPT,
        messages=[
            {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: USER_QUERY}
        ]
    )
    print(response)
elif model == &#34;qwen&#34;:
...
</code></pre><p>The configuration becomes more complicating when providing optionality for self-hosted models or those on AWS Bedrock.</p><p>And with Pydantic AI, we can just switch out the model name:</p><pre tabindex=0><code>from pydantic_ai import Agent

model = &#39;google-gla:gemini-1.5-flash&#39;
model = &#39;anthropic:claude-3-7-sonnet-latest&#39;
selected_model = &#39;bedrock:us.anthropic.claude-3-7-sonnet-20250219-v1:0&#39;

agent = Agent(  
    selected_model,
    system_prompt=SYSTEM_PROMPT,  
)

result = agent.run_sync(&#39;Where does &#34;hello world&#34; come from?&#39;)  
print(result.output)
</code></pre><h2 id=tool-use>Tool Use<a hidden class=anchor aria-hidden=true href=#tool-use>#</a></h2><p>This pleased me enough, but there was more coming when testing tool use. With most of the LLM SDKs, the tool use section of the agent initiation is a complex nest of JSON. With Pydantic AI, I can just throw a decorator above a function, and the agent now has access to this tool. We can also provide context (such as a database connection) to these tools, further empowering the LLMs.</p><p>Here’s a comparison of tool use with the Anthropic SDK compared with Pydantic AI:</p><h3 id=tool-use-with-anthropic-sdk>Tool use with Anthropic SDK:<a hidden class=anchor aria-hidden=true href=#tool-use-with-anthropic-sdk>#</a></h3><pre tabindex=0><code>import anthropic
import json

client = anthropic.Anthropic()

# Python function to get weather from API
def get_weather(location, unit=&#34;fahrenheit&#34;):
    # In reality, you&#39;d call a weather API here
    return {
        &#34;location&#34;: location,
        &#34;temperature&#34;: &#34;72&#34;,
        &#34;unit&#34;: unit,
        &#34;condition&#34;: &#34;sunny&#34;
    }

# tools schema to describe avaialble functions
tools = [
    {
        &#34;name&#34;: &#34;get_weather&#34;,
        &#34;description&#34;: &#34;Get the current weather in a given location&#34;,
        &#34;input_schema&#34;: {
            &#34;type&#34;: &#34;object&#34;,
            &#34;properties&#34;: {
                &#34;location&#34;: {
                    &#34;type&#34;: &#34;string&#34;,
                    &#34;description&#34;: &#34;The city and state, e.g. San Francisco, CA&#34;
                },
                &#34;unit&#34;: {
                    &#34;type&#34;: &#34;string&#34;,
                    &#34;enum&#34;: [&#34;celsius&#34;, &#34;fahrenheit&#34;],
                    &#34;description&#34;: &#34;The unit of temperature to return&#34;
                }
            },
            &#34;required&#34;: [&#34;location&#34;]
        }
    }
]

# Send request to external LLM, providing tools schema
message = client.messages.create(
    model=&#34;claude-3-5-sonnet-20241022&#34;,
    max_tokens=1024,
    tools=tools,
    messages=[
        {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;What&#39;s the weather like in Boston?&#34;}
    ]
)
</code></pre><h3 id=tool-use-with-pydanticai>Tool use with PydanticAI:<a hidden class=anchor aria-hidden=true href=#tool-use-with-pydanticai>#</a></h3><pre tabindex=0><code>from pydantic_ai import Agent, RunContext

agent = Agent(&#39;anthropic:claude-3-5-sonnet-latest&#39;)

@agent.tool_plain  
def get_weather(location, unit=&#34;fahrenheit&#34;):
    &#34;&#34;&#34;Retrieve temperature and conditions for a given location.&#34;&#34;
    return {
        &#34;location&#34;: location,
        &#34;temperature&#34;: &#34;72&#34;,
        &#34;unit&#34;: unit,
        &#34;condition&#34;: &#34;sunny&#34;
    }


weather = agent.run_sync(&#39;What&#39;s the weather like today in San Antonio?&#39;)
</code></pre><p>The decorator design makes adding tools much more straightforward and fun - we can now focus on what capabilities we want to give our agent, and not be bogged down in ensuring the JSON schema is complete for each available tool.</p><p>We are still early in our “tool use” era of LLM development, but it is a joy to work with Pydantic AI so we can focus on the overall program logic and user empowerment.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>While writing this piece I realized that Anthropic and many other LLM providers provide OpenAI-compatible endpoints, which removes the need for the <code>if</code> logic and accomodating the various SDK languages. This would have useful to discover earlier, but good to know!&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=http://localhost:1313/posts/tauri-and-redhat/><span class=title>Next »</span><br><span>Why Tauri v2 and Red Hat don’t mix…yet</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>Digital Reservoirs</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>